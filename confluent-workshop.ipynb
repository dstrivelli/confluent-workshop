{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9463ac8-ab73-4666-8fa6-cac9af50da10",
   "metadata": {},
   "source": [
    "# Confluent 101 Workshop\n",
    "\n",
    "<img src=\"images/jupyter-setup.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cc6bf7-1667-4101-8562-292a5afb649d",
   "metadata": {},
   "source": [
    "# What is Confluent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85814937-d740-4db1-bf5a-0f3cac9aa1ca",
   "metadata": {},
   "source": [
    "<img src=\"images/confluent-logo.png\" width=\"400\" height=\"300\"> \n",
    "Confluent is a company that provides a platform built around Apache Kafka, a popular open-source distributed streaming platform. At its core, Confluent aims to empower organizations to harness the full potential of real-time data streams, enabling them to build scalable, event-driven architectures that drive business innovation.\n",
    "\n",
    "## Key Components:\n",
    "\n",
    "### 1. Apache Kafka:\n",
    "   Apache Kafka is a distributed event streaming platform that allows users to publish, subscribe to, store, and process streams of records in real-time. It is known for its scalability, fault-tolerance, and durability, making it suitable for a wide range of use cases, from real-time analytics to data integration.\n",
    "\n",
    "### 2. Confluent Platform:\n",
    "<img src=\"images/cp-demo-overview.jpg\" width=\"1200\" height=\"500\"> \n",
    "   Confluent Platform is an enterprise-grade distribution of Apache Kafka, enhanced with additional features and tools to simplify deployment, management, and monitoring of Kafka clusters. It includes features such as Schema Registry, Connectors, Control Center, and ksqlDB, providing comprehensive capabilities for building end-to-end streaming applications.\n",
    "\n",
    "### 3. Ecosystem Integrations:\n",
    "   Confluent offers a rich ecosystem of integrations and connectors that enable seamless integration with other data systems, databases, cloud services, and event sources. This allows organizations to leverage Kafka as the central nervous system for their data infrastructure, enabling data to flow freely across the entire organization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e40c8-f391-4668-b7bc-ebbc25f68b62",
   "metadata": {},
   "source": [
    "# What is Kafka?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b111572a-cd30-45e6-b496-9492d13fd725",
   "metadata": {},
   "source": [
    "<img src=\"images/kafka-logo.png\" width=\"400\" height=\"300\"> \n",
    "Apache Kafka is a distributed event streaming platform designed to handle high-volume, real-time data streams. It serves as a central nervous system for modern applications, enabling the processing and analysis of continuous data flows at scale. Kafka is built for reliability, scalability, and fault-tolerance, making it suitable for a wide range of use cases across industries.\n",
    "\n",
    "## Key Capabilities:\n",
    "\n",
    "### 1. Publish-Subscribe Messaging:\n",
    "   Kafka follows a publish-subscribe messaging model, where producers publish messages to topics, and consumers subscribe to these topics to receive messages. This decouples data producers from consumers, allowing for flexible, asynchronous communication.\n",
    "<img src=\"images/kafka-multitenancy.png\" width=\"800\" height=\"400\"> \n",
    "### 2. Fault-Tolerant Storage:\n",
    "   Kafka provides fault-tolerant storage of data streams using a distributed commit log. Messages are durably stored on disk and replicated across multiple brokers for resilience. This ensures that data is not lost even in the event of hardware failures.\n",
    "<img src=\"images/kafka-broker-topology.png\" width=\"800\" height=\"400\"> \n",
    "\n",
    "### 3. Scalability:\n",
    "   Kafka is designed to scale horizontally to handle massive data volumes and high throughput. It supports partitioning of topics, allowing messages to be distributed across multiple brokers for parallel processing. This enables Kafka clusters to scale seamlessly to accommodate growing data workloads.\n",
    "\n",
    "### 4. Stream Processing:\n",
    "   Kafka supports stream processing capabilities through its Streams API and integration with Apache Kafka Streams. These features enable real-time processing and analysis of data streams directly within the Kafka ecosystem, facilitating the development of complex event-driven applications.\n",
    "<img src=\"images/kafka-decoupling.png\" width=\"800\" height=\"400\"> \n",
    "\n",
    "## Use Cases:\n",
    "\n",
    "### 1. Real-Time Analytics:\n",
    "   Kafka is widely used for real-time analytics applications, including monitoring, dashboards, and operational intelligence. It enables organizations to ingest, process, and analyze large volumes of data streams in real-time, allowing for timely insights and decision-making.\n",
    "\n",
    "### 2. Data Integration:\n",
    "   Kafka serves as a central data hub for integrating disparate data sources and systems within an organization. It facilitates data movement, replication, and synchronization between databases, applications, and services, enabling seamless data integration across the enterprise.\n",
    "\n",
    "### 3. Event-Driven Architectures:\n",
    "   Kafka is a fundamental building block for event-driven architectures, where applications react to events and state changes in real-time. It enables event sourcing, event-driven microservices, and event-driven workflows, fostering agility, responsiveness, and scalability in modern application development.\n",
    "\n",
    "### 4. Log and Event Collection:\n",
    "   Kafka is used for log and event collection in distributed systems, infrastructure monitoring, and security analytics. It acts as a centralized platform for collecting, aggregating, and analyzing logs, metrics, and events from diverse sources, facilitating observability and troubleshooting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2a14fc-3a38-4aab-9fc4-06eabb6bb118",
   "metadata": {},
   "source": [
    "# Agenda - Day 1\n",
    "- Get connected\n",
    "- Kafka\n",
    "- Schema Registry\n",
    "- RBAC\n",
    "- Connect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e6ab4-fcf4-42cc-a23c-43ef5be2bee0",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Helpful Links\n",
    "- [BIP Confluent Control Center](https://bip-controlcenter.bip-sbx-ec4ac126.eks.local)\n",
    "- [BIP AKHQ](https://akhq.bip-sbx-ec4ac126.eks.local/ui/bip-kafka)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6a014-ab85-4698-a5c2-8e03ab497541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 01 - click me and press (shift + enter) to run me\n",
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8acc0f-3a6f-46ea-b783-ed3a7d9f0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 02\n",
    "a = 1\n",
    "b = 10\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cdb6a0-b797-4956-bed1-9818f32d8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 03\n",
    "a = a+1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45430eab-03f8-4696-a653-0f843490254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 04\n",
    "!pip install confluent-kafka fastavro prometheus-client faker kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2b45e-6086-4cf2-a62f-dd6792b3b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 05\n",
    "from prometheus_client import start_http_server, Gauge\n",
    "from confluent_kafka import DeserializingConsumer, Consumer, Producer, TopicPartition, SerializingProducer, ConsumerGroupState\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient, Schema\n",
    "from confluent_kafka.schema_registry.avro import AvroDeserializer, AvroSerializer\n",
    "from confluent_kafka.serialization import StringDeserializer\n",
    "from IPython.display import display, Image\n",
    "from IPython.display import Markdown as md\n",
    "from pprint import pprint as pp\n",
    "from jinja2 import Template\n",
    "from faker import Faker\n",
    "from helpers import render_from_template, admin_create_topic, admin_delete_topic, send_data_to_topic, consume, get_image\n",
    "from helpers import list_confluent_rolebindings, create_confluent_rolebinding, create_confluent_rolebinding_schema\n",
    "import getpass\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f6a47-ac3a-4b72-8af4-2135feb77be6",
   "metadata": {},
   "source": [
    "### Let's get your credentials setup for reuse.\n",
    "_**Note, your password will be hidden from the screen, but will be saved into a variable. Do not print the `_password` variable to the screen.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439461d-5c7f-4312-8a71-f198bf5f6e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 06\n",
    "_username = input(\"Enter your IDM username (SBX)\")\n",
    "_password = getpass.getpass(\"Enter your IDM password (SBX)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a8e157-f04d-4d70-b850-bcd6ec4f3a0a",
   "metadata": {},
   "source": [
    "# Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d483244-6d4f-4e92-adc0-421a96663aa6",
   "metadata": {},
   "source": [
    "## Key Terms\n",
    "\n",
    "<img src=\"images/kafka-event-anatomy.png\" width=\"600\" height=\"300\"> \n",
    "\n",
    "- **Broker**:\n",
    "   A Kafka broker is a single instance of a Kafka server that stores and manages topic partitions. Brokers are responsible for receiving messages from producers, storing them on disk, and serving them to consumers.\n",
    "\n",
    "- **Topic**:\n",
    "   A Kafka topic is a logical category or feed name to which messages are published by producers. Topics are partitioned and replicated across multiple brokers for scalability and fault-tolerance.\n",
    "\n",
    "- **Partition**:\n",
    "    A Kafka topic is divided into one or more partitions, which are the basic unit of parallelism and scalability in Kafka. Each partition is an ordered, immutable sequence of messages and is hosted by exactly one broker in the Kafka cluster. Partitions allow Kafka to scale horizontally and distribute message processing across multiple brokers.\n",
    "\n",
    "- **Key**:\n",
    "   In Kafka, a key is an optional attribute associated with each message that is used for message routing and partitioning. When a message is produced to a topic with key-based partitioning, Kafka uses the key to determine the partition to which the message will be sent.\n",
    "\n",
    "- **Value**:\n",
    "   The value in Kafka refers to the actual payload or data of the message that is published to a topic by a producer. It represents the information being transmitted from the producer to the consumer.\n",
    "\n",
    "- **Header**:\n",
    "   Headers in Kafka are key-value pairs associated with each message that provide additional metadata or contextual information about the message. Headers can be used to store information such as message timestamps, message headers, or any custom metadata relevant to the application.\n",
    "\n",
    "- **Offset**:\n",
    "   An offset is a unique identifier assigned to each message within a partition of a Kafka topic. Offsets are used by consumers to keep track of their progress in reading messages from a topic. Each message in a partition has a monotonically increasing offset starting from 0.\n",
    "\n",
    "- **Producer**:\n",
    "   A Kafka producer is an application or process that publishes messages to Kafka topics. Producers are responsible for creating messages and sending them to Kafka brokers for storage and distribution to consumers.\n",
    "\n",
    "- **Consumer**:\n",
    "   A Kafka consumer is an application or process that subscribes to one or more topics and reads messages from them. Consumers can be part of a consumer group, where each consumer in the group reads messages from a subset of the partitions of the subscribed topics.\n",
    "\n",
    "- **Consumer Group**:\n",
    "   A consumer group is a group of consumer instances that collectively consume messages from one or more topics. Each consumer group divides the partitions of the subscribed topics among its members, ensuring that each message is consumed by only one consumer within the group.\n",
    "\n",
    "- **Event**:\n",
    "   An event, in the context of Confluent or Kafka refers to an individual message that is produced to or consumed from a Kafka topic. These are often referred to as messages, records, or events interchangeably.\n",
    "\n",
    "<img src=\"images/kafka-one-topic-multi-consumer.png\" width=\"600\" height=\"300\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175832c-8bb8-4a7c-bca3-4b8da646c86c",
   "metadata": {},
   "source": [
    "### Admin Client\n",
    "The Kafka Admin Client is a Java client library used for managing and administering Apache Kafka clusters. It provides an interface to perform various administrative tasks such as creating, listing, deleting topics, managing consumer groups, and querying metadata about brokers, topics, and partitions.\n",
    "\n",
    "The Admin Client is typically used by administrators or system operators to perform administrative tasks programmatically, rather than using command-line tools like `kafka-topics` or `kafka-configs`. This allows for automation and integration with other systems.\n",
    "\n",
    "Some common tasks performed using the Admin Client include:\n",
    "- Listing topics in a Kafka cluster.\n",
    "- Creating and deleting topics.\n",
    "- Altering topic configurations.\n",
    "- Describing topic properties.\n",
    "- Adding and removing partitions.\n",
    "- Listing consumer groups and their offsets.\n",
    "\n",
    "Using the Admin Client, developers can programmatically manage Kafka clusters, making it easier to automate administrative tasks and integrate Kafka with other systems and workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8023735-4116-4d53-b439-b253fadd4a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 07\n",
    "# Set our Kafka bootstrap server\n",
    "bootstrap_servers = 'bip-kafka.confluent.svc.cluster.local:9092'\n",
    "broker_config = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanism': 'PLAIN',\n",
    "    'sasl.username': _username,\n",
    "    'sasl.password': _password\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f656c2bf-1ea1-4489-af3f-8c1050ae2b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 08\n",
    "# Create AdminClient instance\n",
    "admin_client = AdminClient(broker_config)\n",
    "\n",
    "# List topics\n",
    "topics = admin_client.list_topics().topics\n",
    "topics_cleaned = [k for k,v in topics.items() if k.find('confluent') < 0]\n",
    "\n",
    "# Print the list of topics\n",
    "display(topics_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2b04b0-f1e3-4e27-b87a-235a36460253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 09\n",
    "# Create a new topic\n",
    "topic_name = f\"DEMO_TOPIC_{_username.upper()}\"\n",
    "topic_partitions = 1\n",
    "topic_replication_factor = 1\n",
    "\n",
    "new_topic = NewTopic(topic_name, topic_partitions, topic_replication_factor)\n",
    "admin_client.create_topics([new_topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdffc72-612c-4bdc-8e7f-b466d795e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 10\n",
    "# List topics\n",
    "topics = admin_client.list_topics().topics\n",
    "topics_cleaned = [k for k,v in topics.items() if \"DEMO_TOPIC\" in k]\n",
    "\n",
    "# Print the list of topics\n",
    "display(topics_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e375766-9814-43f3-8555-5b1d2c9d8180",
   "metadata": {},
   "source": [
    "### Producer\n",
    "In Apache Kafka, a producer is a client application that publishes (produces) records to Kafka topics. It sends data in the form of records to Kafka brokers, which then stores and distributes these records across partitions of the topic.\n",
    "\n",
    "Producers are typically responsible for:\n",
    "- Determining which topic to publish messages to.\n",
    "- Serializing the message data into bytes.\n",
    "- Assigning a partition key or letting Kafka do it automatically.\n",
    "- Sending the records to Kafka brokers for distribution.\n",
    "\n",
    "Now, let's set up a basic Confluent Kafka producer using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7dda4-3d49-434f-afb9-d0a9e74fefc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 11\n",
    "producer_config = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanism': 'PLAIN',\n",
    "    'sasl.username': _username,\n",
    "    'sasl.password': _password\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad3e09-e4a7-4fa9-9224-4a259678ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 12\n",
    "# Create a Kafka producer instance\n",
    "try:\n",
    "    producer = Producer(producer_config)\n",
    "    print(\"Successfully created producer!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9d88ec-5efe-4722-9258-3b52506567ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 13\n",
    "message = input(f\"Enter a message to write to the {topic_name} Kafka topic:\")\n",
    "producer.produce(topic_name, message)\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8342afe-cc2d-4ca9-a4b5-3dfab95db4a5",
   "metadata": {},
   "source": [
    "### We have successfully sent a simple message to a Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c02d23e-3040-4b43-8ebe-48e01b7d0de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 14\n",
    "render_from_template(\"consume\", {'topic_name': topic_name, '_username': _username, '_password': _password})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c435a-4080-48ee-a3c9-181ed4992449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 15\n",
    "# List all demo topics\n",
    "display([k for k,v in producer.list_topics().topics.items() if k.find(\"DEMO_TOPIC\") > -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27315f6-0000-4991-ac0a-0fb92f624790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 16\n",
    "friend_topic = \"DEMO_TOPIC_CUCUMBER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34abefd1-247f-4bc2-a78a-cb7e2a80faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 17\n",
    "message = input(f\"Enter a message to write to the {friend_topic} Kafka topic:\")\n",
    "message = f\"({_username.upper()}): {message}\"\n",
    "producer.produce(friend_topic, message)\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da86cd-a85a-44a3-b25d-16492768949e",
   "metadata": {},
   "source": [
    "### What other kind of data can we put into Kafka?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14657f7b-c97f-4f0a-a933-d315d853ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 18\n",
    "images_topic = topic_name + \"_IMAGES\"\n",
    "admin_create_topic(admin_client, images_topic, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e63eb-601a-4fdb-8bba-ddabcf012449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 19\n",
    "new_image = get_image()\n",
    "Image(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd82783-453a-4af6-be6a-36cbd7cae58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 20\n",
    "producer.produce(images_topic, new_image)\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a730d-80b0-4240-aadd-76ea92b34ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 21\n",
    "consumer_config = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanism': 'PLAIN',\n",
    "    'sasl.username': _username,\n",
    "    'sasl.password': _password,\n",
    "    'group.id': f\"DEMO_IMAGES_{_username.upper()}\",\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "consumer = Consumer(consumer_config)\n",
    "consumer.subscribe([images_topic])\n",
    "\n",
    "incoming_images = consume(consumer, 3)\n",
    "consumer.close()\n",
    "recovered_images = [Image(i) for i in incoming_images]\n",
    "display(*recovered_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ae9ba-0df9-495f-8662-bdffc8949a1d",
   "metadata": {},
   "source": [
    "## Kafka Consumer Groups\n",
    "\n",
    "### Overview\n",
    "<img src=\"images/consumer-group-heartbeats.png\" height=300 width=500>\n",
    "\n",
    "A Kafka consumer group is a set of consumers that work together to consume and process messages from one or more Kafka topics. Consumer groups provide scalability and fault tolerance by distributing message processing across multiple consumers.\n",
    "\n",
    "### Joining a Consumer Group\n",
    "To join a consumer group, a Kafka consumer instance must specify the group ID when it subscribes to one or more topics. When a consumer joins a group, it becomes part of the group's consumer pool and is eligible to receive messages from the subscribed topics.\n",
    "\n",
    "### Assignments\n",
    "Once consumers have joined a group, Kafka's group coordinator assigns partitions from the subscribed topics to each consumer within the group. Each partition is assigned to exactly one consumer in the group, ensuring that each message is processed by only one consumer.\n",
    "<img src=\"images/consumer-group-healthy.png\" height=400 width=800>\n",
    "\n",
    "### Heartbeats\n",
    "Consumers within a group periodically send heartbeats to the group coordinator to signal that they are alive and processing messages. Heartbeats prevent the group coordinator from considering a consumer as failed and triggering a rebalance when the consumer is still active.\n",
    "\n",
    "### Rebalancing\n",
    "Rebalancing occurs when the group coordinator detects changes in the group membership or partition assignment. This can happen when a new consumer joins or leaves the group, or when the number of partitions for a topic changes. During a rebalance, the group coordinator reassigns partitions to consumers to ensure an even distribution of workload across the group.\n",
    "\n",
    "<img src=\"images/consumer-group-unhealthy.png\" height=400 width=800>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0ad13-e336-401f-85fb-3a58fc7993ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 22\n",
    "multi_topic_name = topic_name + \"_MULTI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a76965-e60e-4bce-9752-fcfa35bc4a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 23\n",
    "render_from_template(\"consume-multi\", { 'topic_name': multi_topic_name, '_username': _username, '_password': _password})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd6b98-13e4-46ef-b108-bd509f251332",
   "metadata": {},
   "source": [
    "### Two consumers, two partitions\n",
    "- Now we'll go ahead and observe a scenario when there are two consumers pulling against two partitions.\n",
    "- What do you notice about how the partitions are aligned with the consumers?\n",
    "- Do you see any examples where 1 consumer reports on behalf of both partitions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78423bd-a2f7-43d1-9412-d876e404279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 24\n",
    "admin_create_topic(admin_client, multi_topic_name, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa21c74-0316-492e-beac-42bbfbbe57f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 25\n",
    "send_data_to_topic(producer, multi_topic_name, 50, fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e2f05-e979-4278-88ea-5743015f4974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 26\n",
    "admin_delete_topic(admin_client, multi_topic_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e37f5-6ffb-4c64-a474-18693ed2aee8",
   "metadata": {},
   "source": [
    "### Two consumers, ten partitions\n",
    "- Now we'll go ahead and observe a scenario when there are two consumers pulling against ten partitions.\n",
    "- What do you notice about how the partitions are aligned with the consumers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc02ff-8069-4d3d-bed8-d3ad2f0ee89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 27\n",
    "admin_create_topic(admin_client, multi_topic_name, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f87f8-2f8f-4f21-ad5b-32c9024b8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 28\n",
    "send_data_to_topic(producer, multi_topic_name, 50, fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b2297-50e1-452c-9516-664b371b7a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 29\n",
    "admin_delete_topic(admin_client, multi_topic_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6ed1c-39ca-4d7c-8513-647afb08122e",
   "metadata": {},
   "source": [
    "### Two consumers, three partitions\n",
    "- Now we'll go ahead and observe a scenario when there are two consumers pulling against three partitions.\n",
    "- What do you notice about how the partitions are aligned with the consumers?\n",
    "- Do you see any examples where 1 consumer reports on behalf of multiple partitions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f157e4-fb1f-4052-96ca-ae96448489d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 30\n",
    "admin_create_topic(admin_client, multi_topic_name, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45110aae-fb36-4ab9-bd00-42dc51e1f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 31\n",
    "send_data_to_topic(producer, multi_topic_name, 50, fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a33e045-07cd-4640-9c9e-61fd05823d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 32\n",
    "admin_delete_topic(admin_client, multi_topic_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e1c45-d8a1-4660-8d43-376dbebf772c",
   "metadata": {},
   "source": [
    "### Two consumers, one partition\n",
    "- Now we'll go ahead and observe a scenario when there are two consumers pulling against one partition.\n",
    "- What do you notice about how the partitions are aligned with the consumers?\n",
    "- What do you notice is missing?\n",
    "- What can you infer about the number of partitions and how that affects the number of consumers able to be in a group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023fc036-9813-4e6a-b15c-65b3b2827d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 33\n",
    "admin_create_topic(admin_client, multi_topic_name, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280ea45-a0c0-4a2b-bb83-bc6329246335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 34\n",
    "send_data_to_topic(producer, multi_topic_name, 50, fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9994138e-13e0-4141-b36b-ddfdf99af3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 35\n",
    "admin_delete_topic(admin_client, multi_topic_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaff883-0d32-4b67-a11d-a7d7caa66280",
   "metadata": {},
   "source": [
    "### Consumer Re-balancing\n",
    "- Now we'll go ahead and observe a similar scenario\n",
    "- Let's go back to a two partition setup\n",
    "- Launch 2 terminals, within each terminal, run the `python consume-split.py` command.\n",
    "- What do you notice about how the partitions are aligned with the consumers?\n",
    "- Use `ctrl-c` in one terminal to kill the consumer\n",
    "- What changed?\n",
    "- Resume that dead consumer by running `python consume-split.py` again\n",
    "- What behaviors did you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e075a-0702-496f-a4d8-c179e2e298ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 36\n",
    "render_from_template(\"consume-split\", { 'topic_name': multi_topic_name, '_username': _username, '_password': _password })\n",
    "admin_create_topic(admin_client, multi_topic_name, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bfb2b9-323b-4c26-9f46-d53675dcd44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 37\n",
    "send_data_to_topic(producer, multi_topic_name, 50, fake=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bb71f-61e2-4c2d-b46f-d885458d2789",
   "metadata": {},
   "source": [
    "## Kafka Message Deletion\n",
    "\n",
    "Kafka offers two main deletion modes for managing message retention and cleanup: log compaction and message deletion. Each deletion mode serves different use cases and offers unique benefits.\n",
    "\n",
    "### Message Deletion (config.policy = delete)\n",
    "Message deletion is a retention policy in Kafka that removes messages from a topic based on configurable criteria such as retention time or size. This mode allows for the deletion of old or obsolete messages to free up storage space and manage data lifecycle.\n",
    "\n",
    "#### Use Cases:\n",
    "- **Data Archival**: Message deletion is often used for managing data retention policies and archiving historical data. By specifying a retention period, organizations can automatically delete old messages to comply with regulatory requirements or minimize storage costs.\n",
    "- **Transient Data**: Message deletion is suitable for managing transient data or ephemeral streams where only recent data is relevant. This mode allows organizations to prioritize storage resources for current data while discarding older messages.\n",
    "\n",
    "### Log Compaction (config.policy = compact)\n",
    "Log compaction is a retention policy in Kafka that preserves the latest value for each key within a topic while discarding older values. This mode ensures that the latest state for each key is always available for consumers, even in the presence of large volumes of data.\n",
    "\n",
    "#### Use Cases:\n",
    "- **Change Data Capture (CDC)**: Log compaction is commonly used in CDC pipelines to capture and replicate changes from source systems to downstream consumers. By retaining only the latest state for each key, log compaction ensures that consumers receive an accurate representation of the source data's current state.\n",
    "- **Key-Value Storage**: Log compaction is suitable for maintaining key-value stores where only the latest value for each key needs to be retained. This mode enables efficient storage and retrieval of key-value pairs with minimal overhead.\n",
    "\n",
    "[Ephemeral SBX AKHQ](https://akhq.bip-sbx-ec4ac126.eks.local/ui/bip-kafka/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061b1b5-8216-47f7-9c88-a2f91964959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 38\n",
    "delete_topic = f\"{topic_name}_DELETE\"\n",
    "compact_topic = f\"{topic_name}_COMPACT\"\n",
    "\n",
    "admin_create_topic(admin_client, topics=[\n",
    "    NewTopic(topic=delete_topic, num_partitions=1, replication_factor=1, config={\"cleanup.policy\": \"delete\", \"retention.ms\": \"10000\"}),\n",
    "    NewTopic(topic=compact_topic, num_partitions=1, replication_factor=1, config={\"cleanup.policy\": \"compact\", \"delete.retention.ms\": \"10000\"})\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e860b5-884a-4d7c-8d08-03c739a6fc6f",
   "metadata": {},
   "source": [
    "### config.policy = delete\n",
    "- For this test, let's send some data to our `delete_topic`\n",
    "- Via AKHQ, observe the messages on your topic\n",
    "- Wait 10-30 seconds and refresh the topic view in AKHQ\n",
    "- What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7988736-e70a-4cc2-8564-b6598fbe72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 39\n",
    "send_data_to_topic(producer, delete_topic, 50, fake=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97bb03b-450a-40ca-a84b-3e62b0d9a436",
   "metadata": {},
   "source": [
    "### config.policy = compact\n",
    "- For this test, let's send some data to our `compact_topic`\n",
    "- Via AKHQ, observe the messages on your topic\n",
    "- Wait 10-30 seconds and refresh the topic view in AKHQ\n",
    "- What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6efc8-1bbc-48bb-a2f6-acd0a1432653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 40\n",
    "producer.produce(compact_topic, key=b'key1', value=b'apple')\n",
    "producer.produce(compact_topic, key=b'key2', value=b'apple')\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d12e7e4-f11a-456b-a625-150cc5dbd52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 41\n",
    "producer.produce(compact_topic, key=b'key1', value=b'banana')\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3349722-9f29-4fc5-b078-e49d2f1f38d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 42\n",
    "producer.produce(compact_topic, key=b'key2', value=None)\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759d77b-37a6-4f4d-84ce-76bf6239724b",
   "metadata": {},
   "source": [
    "## Compaction Troubleshooting\n",
    "If your Kafka topic is not compacting messages even after the configured `delete.retention.ms` time has passed, there could be several reasons for this behavior. Here are some potential issues to investigate:\n",
    "\n",
    "**Configuration Mismatch:**\n",
    "Ensure that the `delete.retention.ms` configuration is applied correctly at both the topic level and the broker level. Verify the topic configuration using the `describe_topics()` method of the admin client, and also check the broker configuration to ensure consistency.\n",
    "\n",
    "**Reconciliation Time:**\n",
    "Understand that Kafka's log compaction process may not trigger immediately after the retention time has passed. There is a reconciliation process that occurs periodically (controlled by `log.cleaner.backoff.ms` and `log.cleaner.min.cleanable.ratio` configurations) to determine when to perform log compaction. If there are not enough eligible log segments for compaction, it may delay the process.\n",
    "\n",
    "**Segment Size:**\n",
    "Check the size of the log segments in your topic. Log compaction is triggered on a per-segment basis, so if your segments are large and do not meet the `min.cleanable.dirty.ratio` threshold, they may not be compacted until they reach that threshold.\n",
    "\n",
    "**Consumer Lag:**\n",
    "If there are active consumers reading from the topic, Kafka may delay log compaction until the lagging consumers catch up with the latest messages. Ensure that your consumers are not lagging behind significantly, as this can affect the compaction process.\n",
    "\n",
    "**Broker Load:**\n",
    "High broker load or resource constraints may impact the log compaction process. Monitor the broker's CPU, memory, and disk usage to ensure that it has enough resources to perform compaction efficiently.\n",
    "\n",
    "**Topic Segment Override:**\n",
    "Check if there are any topic-level overrides that could be affecting the log compaction behavior. Ensure that there are no conflicting configurations at the topic level that override the `delete.retention.ms` setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c8e271-af73-474f-9c58-2564de628e7b",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac1b7dc-8952-41e6-a743-bfbd48b12c98",
   "metadata": {},
   "source": [
    "# Schemas and Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72cae4-88e8-4d73-b78d-91240b0aa9f1",
   "metadata": {},
   "source": [
    "\n",
    "## Importance of Data Structure:\n",
    "\n",
    "Data in Kafka needs to be structured and well-defined to ensure interoperability and compatibility across different systems and applications. Without a clear understanding of the data structure, it becomes challenging to process, interpret, and analyze the data effectively.\n",
    "\n",
    "## Serialization and Deserialization:\n",
    "\n",
    "Serialization is the process of converting data from its native format into a byte stream, suitable for transmission or storage. Deserialization is the reverse process, converting the byte stream back into its original format.\n",
    "\n",
    "<img src=\"images/serdes-topology.png\" width=\"600\" height=\"300\">\n",
    "\n",
    "### Why Serialization and Deserialization Matter:\n",
    "\n",
    "- **Interoperability:** Serialized data can be transmitted and consumed by different systems, regardless of their programming languages or platforms.\n",
    "  \n",
    "- **Efficiency:** Serialized data is more compact and efficient for transmission and storage compared to raw data formats.\n",
    "  \n",
    "- **Compatibility:** Deserialization ensures that data can be reconstructed accurately, maintaining compatibility across different systems.\n",
    "\n",
    "## Serialization and Deserialization in Confluent Kafka:\n",
    "\n",
    "Confluent provides three different methods for serialization and deserialization:\n",
    "\n",
    "### 1. Externalized Schema:\n",
    "\n",
    "- **Description:** In this approach, the schema is decided externally by the producer and consumer applications. There is no enforcement of schema compatibility or validation by Kafka.\n",
    "  \n",
    "- **Pros:** Flexibility for applications to define and evolve schemas independently.\n",
    "  \n",
    "- **Cons:** Lack of schema enforcement can lead to compatibility issues and data inconsistencies.\n",
    "\n",
    "### 2. Embedded Schema (Registryless):\n",
    "\n",
    "- **Description:** With embedded schema serialization, the schema is included with each message sent to Kafka. Producers encode the schema along with the message data.\n",
    "  \n",
    "- **Pros:** Ensures that the schema is always available with the message, simplifying deserialization.\n",
    "  \n",
    "- **Cons:** Increases message size due to the inclusion of the schema with each message.\n",
    "\n",
    "### 3. Schema Registry:\n",
    "\n",
    "- **Description:** Confluent Schema Registry is a centralized service that stores and manages schemas independently of producer and consumer applications. Producers only send a unique identifier (schema ID) with each message, and consumers retrieve the schema from the registry.\n",
    "  \n",
    "- **Pros:** Promotes schema reuse, centralizes schema management, and ensures schema evolution and compatibility.\n",
    "  \n",
    "- **Cons:** Requires additional infrastructure (Schema Registry) and introduces network overhead for schema retrieval.\n",
    "\n",
    "<img src=\"images/schema-methods.png\" width=800>\n",
    "\n",
    "## Supported Schema Formats:\n",
    "\n",
    "Confluent Schema Registry supports the following schema formats:\n",
    "\n",
    "- **JSON Schema**\n",
    "- **Protocol Buffers (Protobuf)**\n",
    "- **Avro**\n",
    "\n",
    "## Example Schema (Avro):\n",
    "In this Avro schema example, we define a User record with three fields: id (integer), username (string), and email (string). This schema can be used to serialize and deserialize data representing user information in Kafka messages.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"User\",\n",
    "  \"namespace\": \"com.example\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"id\", \"type\": \"int\"},\n",
    "    {\"name\": \"username\", \"type\": \"string\"},\n",
    "    {\"name\": \"email\", \"type\": \"string\"}\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17189fb8-d544-4783-a9ea-f52449d1057e",
   "metadata": {},
   "source": [
    "## Example (Embedded Schema)\n",
    "Now let's walk through an example. We have already demonstrated producing and consuming data to and from a Kafka topic without using a defined schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137fae15-8b63-4265-a795-ecad2b6d9c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 43\n",
    "embedded_schema_topic_name = topic_name + \"_EMBED\"\n",
    "admin_create_topic(admin_client, embedded_schema_topic_name, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2c5fc-127f-4a32-b581-c10e24aeed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 44\n",
    "import io\n",
    "import json\n",
    "from avro.schema import parse\n",
    "import avro.io\n",
    "\n",
    "# Step 1: Define an Avro schema in a Python dictionary\n",
    "schema_dict = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"namespace\": \"com.example\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"int\"},\n",
    "        {\"name\": \"username\", \"type\": \"string\"},\n",
    "        {\"name\": \"email\", \"type\": \"string\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Step 2: Encode the schema into a byte string\n",
    "schema_bytes = json.dumps(schema_dict).encode('utf-8')\n",
    "\n",
    "# Step 3: Use the schema to encode a message into a byte string\n",
    "message_data = {\"id\": 123, \"username\": \"john_doe\", \"email\": \"john.doe@example.com\"}\n",
    "# Assuming you have an Avro schema object 'avro_schema'\n",
    "avro_schema = parse(json.dumps(schema_dict))\n",
    "\n",
    "output_stream = io.BytesIO()\n",
    "encoder = avro.io.BinaryEncoder(output_stream)\n",
    "writer = avro.io.DatumWriter(avro_schema)\n",
    "writer.write(message_data, encoder)\n",
    "message_bytes = output_stream.getvalue()\n",
    "\n",
    "# Step 4: Compose the payload\n",
    "magic_byte = b'\\x00'\n",
    "payload = schema_bytes + magic_byte + message_bytes\n",
    "\n",
    "print(payload)\n",
    "\n",
    "send_data_to_topic(producer, embedded_schema_topic_name, 1, fake=False, messages=[payload])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029b587-57b7-4db5-a20f-6295ca3fd180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 45\n",
    "consumer_config = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanism': 'PLAIN',\n",
    "    'sasl.username': _username,\n",
    "    'sasl.password': _password,\n",
    "    'group.id': f\"DEMO_EMBED_{_username.upper()}\",\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "consumer = Consumer(consumer_config)\n",
    "consumer.subscribe([embedded_schema_topic_name])\n",
    "incoming_messages = consume(consumer, 5)\n",
    "consumer.close()\n",
    "raw_message = incoming_messages[0]\n",
    "raw_split = raw_message.split(b'\\x00')\n",
    "schema = raw_split[0]\n",
    "encoded_message = raw_split[1]\n",
    "avro_schema = parse(schema)\n",
    "decoder = avro.io.BinaryDecoder(io.BytesIO(encoded_message))\n",
    "reader = avro.io.DatumReader(avro_schema)\n",
    "decoded_message = reader.read(decoder)\n",
    "decoded_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3831aa-3184-45cf-a163-51f407fd3e99",
   "metadata": {},
   "source": [
    "### Example (Schema Registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a8cee-447b-4c62-8cf3-536b85ad6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 46\n",
    "registry_schema_topic_name = topic_name + \"_REGISTRY\"\n",
    "admin_create_topic(admin_client, registry_schema_topic_name, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90b487-8f9a-4e81-b50e-347f9548409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 47\n",
    "# Initialize Schema Registry client\n",
    "schema_registry_url = 'https://bip-schemaregistry.confluent.svc.cluster.local:8081'\n",
    "schema_registry_client = SchemaRegistryClient({ \n",
    "        'url': schema_registry_url,\n",
    "        'basic.auth.user.info': f'{_username}:{_password}'\n",
    "    })\n",
    "\n",
    "# Register a schema\n",
    "schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"namespace\": f\"com.example.{_username.lower()}\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"int\"},\n",
    "        {\"name\": \"username\", \"type\": \"string\"},\n",
    "        {\"name\": \"email\", \"type\": \"string\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "fq_schema = Schema(json.dumps(schema), \"AVRO\")\n",
    "\n",
    "subject = registry_schema_topic_name + \"-value\"\n",
    "schema_id = schema_registry_client.register_schema(subject, fq_schema)\n",
    "\n",
    "print(\"Schema registered successfully with ID:\", schema_id)\n",
    "\n",
    "# Retrieve a schema by ID\n",
    "retrieved_schema = schema_registry_client.get_schema(schema_id)\n",
    "display(json.loads(retrieved_schema.schema_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e4f68-f1db-463b-b8fb-fab64b4d4e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 48\n",
    "value_avro_serializer = AvroSerializer(schema_registry_client = schema_registry_client, schema_str = fq_schema.schema_str)\n",
    "\n",
    "# Initialize Serializing Producer\n",
    "conf = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'value.serializer': value_avro_serializer,\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanism': 'PLAIN',\n",
    "    'sasl.username': _username,\n",
    "    'sasl.password': _password\n",
    "}\n",
    "serial_producer = SerializingProducer(conf)\n",
    "\n",
    "message_data = {\n",
    "    \"id\": 456,\n",
    "    \"username\": _username,\n",
    "    \"email\": \"john.foe@example.com\"\n",
    "}\n",
    "\n",
    "serial_producer.produce(topic=registry_schema_topic_name, value=message_data)\n",
    "serial_producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e8fed-a7b8-452e-b38c-e3902898b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 49\n",
    "value_avro_deserializer = AvroDeserializer(schema_registry_client = schema_registry_client)\n",
    "\n",
    "# Initialize Deserializing Consumer\n",
    "conf = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'value.deserializer': value_avro_deserializer,\n",
    "    'group.id': f\"DEMO_REGISTRY_{_username.upper()}\",\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanism': 'PLAIN',\n",
    "    'sasl.username': _username,\n",
    "    'sasl.password': _password,\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "deserial_consumer = DeserializingConsumer(conf)\n",
    "deserial_consumer.subscribe([registry_schema_topic_name])\n",
    "incoming_messages = consume(deserial_consumer, 10)\n",
    "deserial_consumer.close()\n",
    "incoming_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe935a5f-25fb-41ec-b484-4047f39a257a",
   "metadata": {},
   "source": [
    "#### Check the contents of the topic in AKHQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b58730d-8e6f-48ac-9b1e-f9a52919a828",
   "metadata": {},
   "source": [
    "#### Check the contents of the topic in ControlCenter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb2fc26-a865-4178-a927-e474facdff28",
   "metadata": {},
   "source": [
    "# BREAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb21fdb3-3044-463b-bcc2-178a5cac0b94",
   "metadata": {},
   "source": [
    "# Role Based Access Control (RBAC)\n",
    "\n",
    "<img src=\"images/rbac-authentication-overview.png\" height=400 width=1200>\n",
    "\n",
    "## Confluent Role-Based Access Control (RBAC) and Metadata Service (MDS)\n",
    "\n",
    "Confluent Role-Based Access Control (RBAC) is a security feature that provides fine-grained access control to Confluent components such as brokers, schema registry, connectors, and ksqlDB. RBAC ensures that only authorized users or applications can access and perform actions on Confluent resources.\n",
    "\n",
    "### Metadata Service (MDS)\n",
    "The Metadata Service (MDS) serves as the central authority for managing access control policies in a Confluent cluster. MDS controls access to all Confluent components and enforces role-based permissions based on user roles and privileges.\n",
    "\n",
    "### MDS Functions as a Middleman\n",
    "MDS acts as an intermediary between users or applications and Confluent components. It handles authentication, authorization, and role management, ensuring that only authenticated and authorized users can access Confluent resources.\n",
    "\n",
    "### IDM LDAP Server Integration\n",
    "MDS is often integrated with an Identity Management (IDM) LDAP server, which serves as the user directory for managing user identities, groups, and roles. The IDM LDAP server provides a centralized repository of user information, allowing MDS to authenticate users and enforce access control policies based on LDAP user attributes and group memberships.\n",
    "\n",
    "### Access Control for Confluent Components\n",
    "With MDS, access control policies can be defined at a granular level for Confluent components such as brokers, schema registry, connectors, and ksqlDB. Administrators can assign roles to users or groups, specifying the actions they are allowed to perform on specific resources.\n",
    "\n",
    "Confluent RBAC and MDS provide a robust security framework for protecting sensitive data and ensuring compliance with regulatory requirements in Confluent deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae3d5f-a72a-4faf-a56f-7a07c41fd9de",
   "metadata": {},
   "source": [
    "## Lets continue with an IDM service account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41168f80-07b1-48ac-8518-0b70e356ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 50\n",
    "_sa_username = input(\"Enter service account IDM username (SBX)\")\n",
    "_sa_password = getpass.getpass(\"Enter service account IDM password (SBX)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41f6451-ba9a-4227-b0bb-0f4f68b7287b",
   "metadata": {},
   "source": [
    "### Let's run a test\n",
    "- Use our admin accounts and AdminClient to create a topic\n",
    "- Try to use dumbledore account to produce some data to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a42e9-ab5b-4665-b7bd-93a931790674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 51\n",
    "bootstrap_servers = 'bip-kafka.confluent.svc.cluster.local:9092'\n",
    "broker_config = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanism': 'PLAIN',\n",
    "    'sasl.username': _username, # <--- Admin User (you)\n",
    "    'sasl.password': _password\n",
    "}\n",
    "admin_client = AdminClient(broker_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0856c67-5fdd-40dc-b041-62d4e19b252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 52\n",
    "rbac_test_topic_name = f\"DEMO_TOPIC_RBAC_{_username.upper()}\"\n",
    "admin_create_topic(admin_client, rbac_test_topic_name, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499a7f6-9d46-4bd2-a0b6-7894fbde7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 53\n",
    "producer_config = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanism': 'PLAIN',\n",
    "    'sasl.username': _sa_username, # <--- Service Account User\n",
    "    'sasl.password': _sa_password\n",
    "}\n",
    "\n",
    "# Create a Kafka producer instance\n",
    "try:\n",
    "    producer = Producer(producer_config)\n",
    "    print(\"Successfully created service account producer!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91f9a8-e319-4377-a8d2-800934562a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 54\n",
    "message = input(f\"Enter a message to write to the {rbac_test_topic_name} Kafka topic:\")\n",
    "producer.produce(rbac_test_topic_name, message)\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6182d0-6580-443d-b9eb-a33d69442351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 55\n",
    "md(f\"\"\"\n",
    "#### Lets add the DeveloperWrite permission for the Topic\n",
    "- Go to [BIP Control Center](https://bip-controlcenter.bip-sbx-ec4ac126.eks.local/access-control/manage/assignments)\n",
    "- Sign in using **YOUR** ({_username}) credentials\n",
    "  - Simulating an operational task you might take or request\n",
    "- Go to Assignments tab\n",
    "- Click on the ID (e.g. `xXStDwdjT6WrSKOj3A35Hw`) next to the `Kafka cluster`\n",
    "- Navigate to the `Topic` tab\n",
    "- Click the `+ Add Role Assignment` button\n",
    "- Fill out the form using the following information:\n",
    "  - **Principal Type:** User\n",
    "  - **Principal Name:** {_sa_username}\n",
    "  - **Role:** DeveloperWrite\n",
    "  - **Pattern Type:** Literal\n",
    "  - **Resource ID:** {rbac_test_topic_name}\n",
    "- Then click the `Save` button\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a464e-6ed2-4f60-805e-b2c360c82b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 56\n",
    "consumer_config = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanism': 'PLAIN',\n",
    "    'sasl.username': _sa_username,\n",
    "    'sasl.password': _sa_password,\n",
    "    'group.id': f\"DEMO_RBAC_{_username.upper()}\",\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "consumer = Consumer(consumer_config)\n",
    "consumer.subscribe([rbac_test_topic_name])\n",
    "\n",
    "incoming_messages = consume(consumer, 5)\n",
    "consumer.close()\n",
    "incoming_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc3254-f4a3-4854-82d1-74d20d6a34d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 57\n",
    "md(f\"\"\"\n",
    "#### Lets add the DeveloperManage permission for the Consumer Group\n",
    "- Go to [BIP Control Center](https://bip-controlcenter.bip-sbx-ec4ac126.eks.local/access-control/manage/assignments)\n",
    "- Sign in using **YOUR** ({_username}) credentials\n",
    "  - Simulating an operational task you might take or request\n",
    "- Go to Assignments tab\n",
    "- Click on the ID (e.g. `xXStDwdjT6WrSKOj3A35Hw`) next to the `Kafka cluster`\n",
    "- Navigate to the `Group` tab\n",
    "- Click the `+ Add Role Assignment` button\n",
    "- Fill out the form using the following information:\n",
    "  - **Principal Type:** User\n",
    "  - **Principal Name:** {_sa_username}\n",
    "  - **Role:** DeveloperManage\n",
    "  - **Pattern Type:** Literal\n",
    "  - **Resource ID:** DEMO_RBAC_{_username.upper()}\n",
    "- Then click the `Save` button\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe308ca-9e87-4bc5-ac56-02a4d89bbc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 58\n",
    "md(f\"\"\"\n",
    "#### Lets add the DeveloperRead permission for the Consumer Group\n",
    "- Go to [BIP Control Center](https://bip-controlcenter.bip-sbx-ec4ac126.eks.local/access-control/manage/assignments)\n",
    "- Sign in using **YOUR** ({_username}) credentials\n",
    "  - Simulating an operational task you might take or request\n",
    "- Go to Assignments tab\n",
    "- Click on the ID (e.g. `xXStDwdjT6WrSKOj3A35Hw`) next to the `Kafka cluster`\n",
    "- Navigate to the `Group` tab\n",
    "- Click the `+ Add Role Assignment` button\n",
    "- Fill out the form using the following information:\n",
    "  - **Principal Type:** User\n",
    "  - **Principal Name:** {_sa_username}\n",
    "  - **Role:** DeveloperRead\n",
    "  - **Pattern Type:** Literal\n",
    "  - **Resource ID:** DEMO_RBAC_{_username.upper()}\n",
    "- Then click the `Save` button\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35f664-8d1b-430f-be29-3cab7eb85c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 59\n",
    "md(f\"\"\"\n",
    "#### Lets add the DeveloperRead permission for the Topic\n",
    "- Go to [BIP Control Center](https://bip-controlcenter.bip-sbx-ec4ac126.eks.local/access-control/manage/assignments)\n",
    "- Sign in using **YOUR** ({_username}) credentials\n",
    "  - Simulating an operational task you might take or request\n",
    "- Go to Assignments tab\n",
    "- Click on the ID (e.g. `xXStDwdjT6WrSKOj3A35Hw`) next to the `Kafka cluster`\n",
    "- Navigate to the `Topic` tab\n",
    "- Click the `+ Add Role Assignment` button\n",
    "- Fill out the form using the following information:\n",
    "  - **Principal Type:** User\n",
    "  - **Principal Name:** {_sa_username}\n",
    "  - **Role:** DeveloperRead\n",
    "  - **Pattern Type:** Literal\n",
    "  - **Resource ID:** {rbac_test_topic_name}\n",
    "- Then click the `Save` button\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826fe833-4706-43fb-b804-554340f341e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d2b742-273a-4810-8830-94414e4e993c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf1eadc-9788-4bff-8b2f-f3886aba261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 60\n",
    "list_confluent_rolebindings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ca8bc-92a7-4d13-ac4d-9775549d84fe",
   "metadata": {},
   "source": [
    "#### Sample ConfluentRoleBinding (Topic Write)\n",
    "\n",
    "```yaml\n",
    "---\n",
    "apiVersion: platform.confluent.io/v1beta1\n",
    "kind: ConfluentRolebinding\n",
    "metadata:\n",
    "  annotations:\n",
    "  finalizers:\n",
    "  - confluentrolebinding.finalizers.platform.confluent.io\n",
    "  name: demo-rbac-dstrivelli-topic-write\n",
    "  namespace: confluent\n",
    "spec:\n",
    "  principal:\n",
    "    name: dumbledore\n",
    "    type: user\n",
    "  resourcePatterns:\n",
    "  - name: DEMO_TOPIC_RBAC_DSTRIVELLI\n",
    "    patternType: LITERAL\n",
    "    resourceType: Topic\n",
    "  role: DeveloperWrite\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224dc28-62f9-4b30-ad8b-7e524f20c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 61\n",
    "create_confluent_rolebinding(\n",
    "    name=f\"demo-rbac-{_username.lower()}-topic-write\",\n",
    "    principal_name='dumbledore',\n",
    "    principal_type='user',\n",
    "    resource_id=rbac_test_topic_name,\n",
    "    pattern_type='LITERAL',\n",
    "    resource_type='Topic',\n",
    "    role='DeveloperWrite',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab469c7-3d64-45f5-858f-f613e95d9daa",
   "metadata": {},
   "source": [
    "### Let's setup a new test\n",
    "- Delete and recreate the RBAC topic\n",
    "- Try to produce Avro-Serialized data to a topic\n",
    "- Try to consume Avro-Serialized data from a topic\n",
    "- Add the missing ConfluentRoleBindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae73b7-6222-4a68-8447-d217765807ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 62\n",
    "admin_delete_topic(admin_client, rbac_test_topic_name)\n",
    "admin_create_topic(admin_client, rbac_test_topic_name, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e17072-df1b-4ccb-aae5-2935fe2220ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 63\n",
    "\n",
    "# Initialize Schema Registry client\n",
    "schema_registry_url = 'https://bip-schemaregistry.confluent.svc.cluster.local:8081'\n",
    "rbac_schema_registry_client = SchemaRegistryClient({ \n",
    "        'url': schema_registry_url,\n",
    "        'basic.auth.user.info': f'{_sa_username}:{_sa_password}'\n",
    "    })\n",
    "\n",
    "# Register a schema\n",
    "schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"namespace\": f\"com.example.rbac.{_username.lower()}\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"int\"},\n",
    "        {\"name\": \"username\", \"type\": \"string\"},\n",
    "        {\"name\": \"email\", \"type\": \"string\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "fq_schema = Schema(json.dumps(schema), \"AVRO\")\n",
    "\n",
    "subject = rbac_test_topic_name + \"-value\"\n",
    "schema_id = rbac_schema_registry_client.register_schema(subject, fq_schema)\n",
    "\n",
    "print(\"Schema registered successfully with ID:\", schema_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8bc787-d13f-4a2e-a1cb-5bbc7348bc7d",
   "metadata": {},
   "source": [
    "### Schema Registry RBAC Role Mappings\n",
    "<img src=\"images/schema-rbac.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3b6f2-8cf5-4c55-b7e6-29c2262c804a",
   "metadata": {},
   "source": [
    "#### Sample ConfluentRoleBinding (Schema Write)\n",
    "```yaml\n",
    "---\n",
    "apiVersion: platform.confluent.io/v1beta1\n",
    "kind: ConfluentRolebinding\n",
    "metadata:\n",
    "  name: demo-rbac-dstrivelli-schema-write\n",
    "  namespace: confluent\n",
    "spec:\n",
    "  clustersScopeByIds:\n",
    "    schemaRegistryClusterId: id_bip-schemaregistry_confluent # <------ Notice!\n",
    "  principal:\n",
    "    name: dumbledore\n",
    "    type: user\n",
    "  resourcePatterns:\n",
    "  - name: DEMO_TOPIC_RBAC_DSTRIVELLI\n",
    "    patternType: PREFIXED\n",
    "    resourceType: Subject\n",
    "  role: DeveloperWrite\n",
    "... \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cabb288-8f1c-461a-aa18-5162e9d6c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 64\n",
    "create_confluent_rolebinding_schema(\n",
    "    name=f\"demo-rbac-{_username.lower()}-schema-write\",\n",
    "    principal_name='dumbledore',\n",
    "    principal_type='user',\n",
    "    resource_id=rbac_test_topic_name,\n",
    "    pattern_type='PREFIXED',\n",
    "    resource_type='Subject',\n",
    "    role='DeveloperWrite',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593b222-987b-4032-b936-6f82c20622ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 65\n",
    "# Avro Serialized Production\n",
    "\n",
    "retrieved_schema = rbac_schema_registry_client.get_schema(schema_id)\n",
    "value_avro_serializer = AvroSerializer(schema_registry_client = rbac_schema_registry_client, schema_str = retrieved_schema.schema_str)\n",
    "\n",
    "# Initialize Serializing Producer\n",
    "conf = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'value.serializer': value_avro_serializer,\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanism': 'PLAIN',\n",
    "    'sasl.username': _sa_username,\n",
    "    'sasl.password': _sa_password\n",
    "}\n",
    "rbac_serial_producer = SerializingProducer(conf)\n",
    "\n",
    "message_data = {\n",
    "    \"id\": 456,\n",
    "    \"username\": _sa_username,\n",
    "    \"email\": \"john.foe@example.com\"\n",
    "}\n",
    "\n",
    "rbac_serial_producer.produce(topic=rbac_test_topic_name, value=message_data)\n",
    "rbac_serial_producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc26cc-48e1-47db-a0fe-e076c47cddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 66\n",
    "# Avro Deserialized Consumption\n",
    "value_avro_deserializer = AvroDeserializer(schema_registry_client = rbac_schema_registry_client)\n",
    "\n",
    "# Initialize Deserializing Consumer\n",
    "conf = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'value.deserializer': value_avro_deserializer,\n",
    "    'group.id': f\"DEMO_RBAC_{_username.upper()}\",\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanism': 'PLAIN',\n",
    "    'sasl.username': _sa_username,\n",
    "    'sasl.password': _sa_password,\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "rbac_deserial_consumer = DeserializingConsumer(conf)\n",
    "rbac_deserial_consumer.subscribe([rbac_test_topic_name])\n",
    "incoming_messages = consume(rbac_deserial_consumer, 3)\n",
    "rbac_deserial_consumer.close()\n",
    "incoming_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49977ff9-8877-4b1a-9dca-ab6b724de5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf24895-f482-4c0a-bd6d-4c7606b375a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a2bff-dfbd-4711-84bc-d0a708699b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a729a1-9605-4c4c-94f8-22931a919984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a5ccb-1111-4162-b3ad-23ed80040d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30873cb-03de-4aa2-94d3-8f9c6f511a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e44028-18a2-4520-9ef2-0ed0b2bf218d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68711093-0e83-4712-a0ec-65cfb84669f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "decf964f-66df-4f9f-9039-a4cb99546c96",
   "metadata": {},
   "source": [
    "# Cleanup! Thanks for joining!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2629db58-cc4f-4658-996e-c4888199b5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer group 'DEMO_CUCUMBER' deleted successfully.\n",
      "Consumer group 'DEMO_IMAGES_CUCUMBER' deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "# cell 67\n",
    "import os\n",
    "\n",
    "# Clean up Python Files\n",
    "files = ['consume.py', 'consume-multi.py', 'consume-split.py']\n",
    "for f in files:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "\n",
    "try:\n",
    "    consumer.close()\n",
    "    deserial_consumer.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Clean up Topics\n",
    "bootstrap_servers = 'bip-kafka.confluent.svc.cluster.local:9092'\n",
    "broker_config = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanism': 'PLAIN',\n",
    "    'sasl.username': _username,\n",
    "    'sasl.password': _password\n",
    "}\n",
    "admin_client = AdminClient(broker_config)\n",
    "topics = admin_client.list_topics().topics\n",
    "topics_cleaned = [k for k,v in topics.items() if k.find(f\"DEMO_TOPIC_{_username.upper()}\") > -1]\n",
    "if len(topics_cleaned) > 0:\n",
    "    admin_client.delete_topics(topics_cleaned)\n",
    "\n",
    "# Clean up Consumer Groups\n",
    "cg_states = [\"UNKNOWN\",\"PREPARING_REBALANCING\",\"COMPLETING_REBALANCING\",\"STABLE\",\"DEAD\",\"EMPTY\"]\n",
    "states = {ConsumerGroupState[state] for state in cg_states}\n",
    "future = admin_client.list_consumer_groups(request_timeout=10, states=states)\n",
    "try:\n",
    "    groupids = []\n",
    "    list_consumer_groups_result = future.result()\n",
    "    for valid in list_consumer_groups_result.valid:\n",
    "        groupids.append(valid.group_id)\n",
    "\n",
    "    filtered_groups = [g for g in groupids if f\"{_username.upper()}\" in g]\n",
    "    for group in filtered_groups:\n",
    "        try:\n",
    "            admin_client.delete_consumer_groups([group])\n",
    "            print(f\"Consumer group '{group}' deleted successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete consumer group '{group}': {e}\")\n",
    "except Exception:\n",
    "    raise\n",
    "\n",
    "# Clean up Schemas\n",
    "schema_registry_url = 'https://bip-schemaregistry.confluent.svc.cluster.local:8081'\n",
    "schema_registry_client = SchemaRegistryClient({ \n",
    "        'url': schema_registry_url,\n",
    "        'basic.auth.user.info': f'{_username}:{_password}'\n",
    "    })\n",
    "\n",
    "subjects = schema_registry_client.get_subjects()\n",
    "filtered_subjects = [s for s in subjects if f\"{_username.upper()}\" in s]\n",
    "for subject in filtered_subjects:\n",
    "    try:\n",
    "        schema_registry_client.delete_subject(subject)\n",
    "        print(f\"Deleted subject '{subject}' successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete subject '{subject}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aace17-ab69-46f9-b442-a1bcf758efa5",
   "metadata": {},
   "source": [
    "# Supporting Links\n",
    "\n",
    "- https://docs.confluent.io/platform/current/security/rbac/index.html#rbac-and-acls\n",
    "- https://www.kafka-streams-book.com/\n",
    "- https://docs.confluent.io/operator/current/co-api.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
